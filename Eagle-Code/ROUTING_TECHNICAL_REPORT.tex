\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{booktabs}
\usepackage{array}
\usepackage{longtable}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[strings]{underscore}

\title{Technical Report: Routing, Confidence, Verification, and Routing Overhead in EAGLE and HASS}
\author{Codebase Analysis Report}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
This report documents how routing is implemented in the EAGLE and HASS repositories, how draft confidence is computed, how verifier acceptance works, and how routing overhead can be quantified. The report is based on direct code inspection of both repositories and analysis of generated benchmark artifacts.
\end{abstract}

\section{Scope and Repositories}
This report compares:
\begin{itemize}
  \item EAGLE codebase: \texttt{Eagle-Code}
  \item HASS codebase: \texttt{Hass-Code}
\end{itemize}

Primary focus:
\begin{itemize}
  \item Step-level dual-head confidence routing
  \item Confidence definition and extraction
  \item Verifier accept/reject logic
  \item Overhead introduced by routing
\end{itemize}

\section{Code Map (What Was Audited)}
\subsection{EAGLE}
\begin{itemize}
  \item Core generation: \texttt{eagle/model/ea_model.py}
  \item Dual routing model: \texttt{eagle/model/ea_model_dual.py}
  \item Draft tree and confidence: \texttt{eagle/model/cnets1.py} (EAGLE-2), \texttt{eagle/model/cnets.py}
  \item Verifier/update logic: \texttt{eagle/model/utils.py}
  \item Dual eval driver: \texttt{eagle/evaluation/gen_dual_head_eval.py}
  \item Routing eval and scripts: \texttt{eagle/evaluation/gen_routing_eval.py}, \texttt{evaluate/run\_dual\_head\_eval.sh}, \texttt{evaluate/run\_routing\_eval.sh}
  \item Confidence analysis: \texttt{eagle/evaluation/gen\_confidence\_analysis.py}
\end{itemize}

\subsection{HASS}
\begin{itemize}
  \item Core generation: \texttt{model/ea\_model.py}
  \item Dual routing model: \texttt{model/ea\_model\_dual.py}
  \item Draft tree and confidence: \texttt{model/cnets.py}
  \item Verifier/update logic: \texttt{model/utils.py}
  \item Dual eval driver: \texttt{evaluation/gen\_dual\_head\_eval.py}
  \item Routing eval and scripts: \texttt{evaluation/routed\_eval.py}, \texttt{scripts/run\_dual\_head\_eval.sh}, \texttt{scripts/eval\_routed.sh}
  \item Confidence analysis: \texttt{evaluation/gen\_confidence\_analysis.py}
\end{itemize}

\section{Shared Decoding Mechanics}
Both systems use speculative decoding with:
\begin{enumerate}
  \item A \textbf{draft model} that proposes a tree of candidate continuations.
  \item A \textbf{verifier model} (base LLM) that scores/accepts candidates.
  \item KV-cache updates to avoid recomputing accepted prefixes.
\end{enumerate}

Core functions (both repos):
\begin{itemize}
  \item Tree proposal: \texttt{topK\_genrate(...)}
  \item Verifier forward on tree: \texttt{tree\_decoding(...)}
  \item Acceptance decision: \texttt{evaluate\_posterior(...)}
  \item Commit accepted tokens and refresh tree: \texttt{update\_inference\_inputs(...)} (single-head path)
\end{itemize}

\subsection{Verifier Acceptance Rule}
In greedy mode (\texttt{logits\_processor is None}), both repos implement:
\begin{itemize}
  \item Compare candidate tokens against verifier argmax at each position.
  \item Accept the longest matching prefix.
  \item If no match, fall back to candidate 0.
\end{itemize}

In sampling mode:
\begin{itemize}
  \item Apply processed verifier distribution.
  \item Sequential stochastic accept/reject over candidate path tokens.
  \item Renormalize when rejecting a token.
\end{itemize}

\section{How Confidence Is Computed}
\subsection{Tree-level confidence used by dual routing}
In both EAGLE and HASS draft tree code:
\begin{itemize}
  \item \texttt{scores\_list} stores cumulative log-probability scores along draft paths.
  \item Selected top paths are indexed by \texttt{top\_scores\_index}.
  \item Confidence is computed as:
\end{itemize}
\[
c_i = \exp(s_i), \quad s_i \in \text{selected cumulative log-probabilities}
\]
and returned as \texttt{draft\_confidences}.

This is implemented in:
\begin{itemize}
  \item EAGLE: \texttt{eagle/model/cnets1.py} (and \texttt{cnets.py})
  \item HASS: \texttt{model/cnets.py}
\end{itemize}

\subsection{Accepted vs rejected confidence logging}
For analysis runs:
\begin{itemize}
  \item EAGLE uses \texttt{eagenerate(..., confidence\_log=True)} and labels draft tokens accepted vs rejected using verifier acceptance length and \texttt{retrieve\_indices}.
  \item HASS confidence analysis extracts per-token draft logprobs (\texttt{return\_logprobs=True}) and maps them to confidence via \(\exp(\log p)\), then labels accepted vs rejected similarly.
\end{itemize}

\section{How Routing Works}
\subsection{EAGLE dual-head routing}
Implemented in \texttt{eagle/model/ea\_model\_dual.py}.

At each decoding step:
\begin{enumerate}
  \item Both heads call \texttt{topK\_genrate(..., return\_confidence=True)}.
  \item Per-head mean confidence is computed over proposed draft tokens.
  \item Router chooses the head with larger mean confidence.
  \item Verifier accepts/rejects from the chosen tree.
  \item Both heads stay synchronized via accepted hidden states.
\end{enumerate}

Decision rule:
\[
h^* = \arg\max_{h \in \{1,2\}} \left( \frac{1}{N}\sum_{i=1}^{N} c_{h,i} \right)
\]

\subsection{HASS dual-head routing}
Implemented in \texttt{model/ea\_model\_dual.py}, algorithmically aligned with EAGLE dual routing:
\begin{enumerate}
  \item Generate draft tree+confidence from both heads.
  \item Compare mean confidences.
  \item Verify chosen tree and commit accepted tokens.
  \item Continue with synchronized accepted hidden states.
\end{enumerate}

\subsection{Other routing modes present}
Secondary routing flows also exist:
\begin{itemize}
  \item EAGLE entropy router: \texttt{eagle/evaluation/gen\_routing\_eval.py}, choosing lowest draft entropy head.
  \item HASS keyword router: \texttt{evaluation/routed\_eval.py}, using heuristic math keyword/pattern matching.
\end{itemize}

These are separate from the step-level dual confidence router.

\section{Verification Procedure (What Is Actually Verified)}
At each speculative iteration:
\begin{enumerate}
  \item Verifier computes logits on tree candidates.
  \item \texttt{evaluate\_posterior} selects best candidate and accepted prefix length.
  \item Accepted tokens are appended to \texttt{input\_ids}.
  \item Corresponding KV cache slices are copied into the active prefix region.
  \item Draft generation restarts from accepted hidden states plus next sampled token.
\end{enumerate}

This means routing affects \textbf{which draft tree is proposed}, but verifier logic remains unchanged.

\section{Empirical Results from Existing Artifacts}
All values below are computed from existing JSONL outputs already present in both repositories.

\subsection{EAGLE: \(\tau = \frac{\text{new\_tokens}}{\text{steps}}\)}
\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
Head & mt\_bench & gsm8k & math\_500 & svamp \\
\midrule
MathInstruct & 2.541 & 4.923 & 5.284 & 4.812 \\
ShareGPT & 3.606 & 3.835 & 3.968 & 3.778 \\
MathInstruct+ShareGPT (dual) & 3.632 & 4.751 & 5.195 & 4.717 \\
\midrule
\(\Delta\) vs best single & +0.026 & -0.172 & -0.089 & -0.095 \\
\bottomrule
\end{tabular}
\caption{EAGLE single-head vs dual confidence routing.}
\end{table}

\subsection{HASS: \(\tau = \frac{\text{new\_tokens}}{\text{steps}}\)}
\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
Head & mt\_bench & gsm8k & math\_500 & svamp \\
\midrule
MathInstruct & 2.371 & 4.960 & 5.420 & 4.935 \\
ShareGPT & 3.889 & 4.165 & 3.975 & 4.092 \\
MathInstruct+ShareGPT (dual) & 3.934 & 4.932 & 5.377 & 4.879 \\
\midrule
\(\Delta\) vs best single & +0.044 & -0.028 & -0.043 & -0.056 \\
\bottomrule
\end{tabular}
\caption{HASS single-head vs dual confidence routing.}
\end{table}

\subsection{Confidence separation quality}
Aggregate accepted vs rejected draft confidence (all 4 benchmarks combined):
\begin{table}[h]
\centering
\begin{tabular}{lrrrr}
\toprule
Model & Mean accepted & Mean rejected & Delta & Counts (acc/rej) \\
\midrule
EAGLE MathInstruct & 0.534534 & 0.046742 & 0.487792 & 395728 / 6255165 \\
EAGLE ShareGPT & 0.496855 & 0.053631 & 0.443224 & 378471 / 7630130 \\
HASS MathInstruct & 0.441771 & 0.178571 & 0.263200 & 281528 / 45484 \\
HASS ShareGPT & 0.428201 & 0.224287 & 0.203914 & 295899 / 55220 \\
\bottomrule
\end{tabular}
\caption{Accepted draft tokens consistently have higher confidence than rejected tokens.}
\end{table}

\subsection{Dual-run throughput (from wall\_time in dual outputs)}
\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
System & mt\_bench & gsm8k & math\_500 & svamp \\
\midrule
EAGLE Tok/s & 53.15 & 78.69 & 89.06 & 83.86 \\
HASS Tok/s & 58.31 & 82.98 & 92.62 & 87.86 \\
\bottomrule
\end{tabular}
\caption{Dual-mode token throughput from recorded \texttt{wall\_time}.}
\end{table}

\section{Routing Overhead}
\subsection{Exact algorithmic overhead}
Let one draft decision be one call to \texttt{\_draft\_best}, i.e., one confidence comparison event.
\begin{itemize}
  \item Single-head: one draft head tree per decision.
  \item Dual-head routing: two draft head trees per decision.
\end{itemize}

Therefore, draft-side call overhead is exactly \(+100\%\) per decision.

\subsection{Measured overhead proxy from completed dual runs}
Using logged dual outputs, extra draft-head calls over single-head equivalent are:
\begin{itemize}
  \item EAGLE: 110{,}588 extra head calls for 514{,}393 generated tokens (\(\approx 214.99\) extra calls / 1k tokens)
  \item HASS: 105{,}089 extra head calls for 509{,}028 generated tokens (\(\approx 206.45\) extra calls / 1k tokens)
\end{itemize}

\subsection{End-to-end runtime overhead formula}
If:
\begin{itemize}
  \item \(T_d\): draft-stage cost per decision for one head
  \item \(T_v\): verifier-stage cost per decision
\end{itemize}
then:
\[
T_{\text{single}} \approx T_d + T_v,\quad
T_{\text{dual}} \approx 2T_d + T_v + \epsilon
\]
\[
\text{Overhead ratio} \approx \frac{T_{\text{dual}}-T_{\text{single}}}{T_{\text{single}}}
\approx \frac{T_d}{T_d + T_v}
\]
where \(\epsilon\) (mean confidence compare) is typically negligible.

\textbf{Important}: exact end-to-end overhead percentage versus single-head wall-clock cannot be computed from current artifacts alone because single-head confidence logs in these folders do not include \texttt{wall\_time} fields.

\section{Reproduction Commands}
\subsection{EAGLE dual run}
\begin{verbatim}
cd /home/zbibm/MOSS---Mixture-of-Speculative-Samplers/Eagle-Code
BASE_MODEL_PATH=/home/zbibm/MOSS---Mixture-of-Speculative-Samplers/base \
EA_MODEL_PATH1=checkpoints/Eagle-MathInstruct_20epochs \
EA_MODEL_PATH2=checkpoints/Eagle-ShareGPT_20epochs \
bash evaluate/run_dual_head_eval.sh
\end{verbatim}

\subsection{HASS dual run}
\begin{verbatim}
cd /home/zbibm/MOSS---Mixture-of-Speculative-Samplers/Hass-Code
BASE_MODEL_PATH=/home/zbibm/MOSS---Mixture-of-Speculative-Samplers/base \
EA_MODEL_PATH1=checkpoints/MathInstruct_20epochs/state_final \
EA_MODEL_PATH2=checkpoints/ShareGPT_20epochs \
bash scripts/run_dual_head_eval.sh
\end{verbatim}

\section{Key Implementation Differences}
\begin{itemize}
  \item EAGLE dual implementation is EAGLE-2 style (\texttt{cnets1.py}) with multi-architecture base support (Llama/Qwen2/Qwen3/Mixtral paths in single-head model class).
  \item HASS dual implementation currently targets Llama architecture in \texttt{model/ea\_model\_dual.py}.
  \item HASS confidence-analysis path reconstructs token confidence from returned logprobs (\(\exp(\log p)\)); EAGLE confidence mode uses native draft confidence logging path in \texttt{EaModel.eagenerate}.
  \item HASS dual generation return signature includes \texttt{accept\_length\_list}; EAGLE dual returns \texttt{(input\_ids, new\_token, idx)} in log mode.
\end{itemize}

\section{Conclusion}
Both EAGLE and HASS dual routers implement the same core strategy: choose, at each speculative step, the draft head with higher mean cumulative path confidence, then let the verifier accept/reject normally. Confidence is consistently discriminative (accepted \(>\) rejected), and routing improves conversational benchmark behavior while remaining close to or below the strongest math-specialist head on pure math benchmarks. Draft-side routing overhead is structurally \(+100\%\) in draft calls; the practical end-to-end impact depends on the draft-vs-verifier compute split.

\end{document}
